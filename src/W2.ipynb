{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jWGjiIDXviD"
      },
      "source": [
        "# W2 Practicals - Supervised Learning\n",
        "### (k-NN and Decision Trees)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aims:\n",
        "* To apply k-NN and decision trees as examples of supervised learning models for classification and regression problems.\n",
        "* To get some insight into the trained models, including the influence of a hyperparameter in an ML model.\n",
        "* To produce some assessable work for this subject.\n",
        "\n"
      ],
      "metadata": {
        "id": "LJASedzsl6L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "aZ00D8y5YM4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSI color\n",
        "class color:\n",
        "  YELLOW_BOLD = '\\033[1;33m'\n",
        "  END = '\\033[0m'"
      ],
      "metadata": {
        "id": "U2jCaC1pFcIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1 Make scatterplots of each dataset so you can see what they look like."
      ],
      "metadata": {
        "id": "TW9HNzQYYF8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sub-Plot Setup\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 10))\n",
        "fig.subplots_adjust(wspace=15)\n",
        "fig.lines.append(plt.Line2D([0.5, 0.5], [0, 1], transform=fig.transFigure, color='black', linewidth=1))\n",
        "\n",
        "# 1st Sub-Plot for W3Classif\n",
        "CSV_FILE_1 = 'w3classif.csv'\n",
        "df1 = pd.read_csv(CSV_FILE_1, header=None)\n",
        "df1.columns = [\"Feature1\", \"Feature2\", \"Label\"]\n",
        "\n",
        "sns.scatterplot(x=df1[\"Feature1\"],\n",
        "                y=df1[\"Feature2\"],\n",
        "                hue=df1[\"Label\"],\n",
        "                palette=\"coolwarm\",\n",
        "                alpha=0.7,\n",
        "                ax=axes[0])\n",
        "axes[0].set_title(\"Scatterplot of W3Classif\")\n",
        "axes[0].set_xlabel(\"Feature 1\")\n",
        "axes[0].set_ylabel(\"Feature 2\")\n",
        "\n",
        "# 2nd Sub-Plot for W3Regr\n",
        "CSV_FILE_2 = 'w3regr.csv'\n",
        "df2 = pd.read_csv(CSV_FILE_2, header=None)\n",
        "df2.columns = [\"Feature1\", \"Feature2\"]\n",
        "\n",
        "sns.scatterplot(x=df2[\"Feature1\"],\n",
        "                y=df2[\"Feature2\"],\n",
        "                alpha=0.7,\n",
        "                ax=axes[1])\n",
        "axes[1].set_title(\"Scatterplot with W3Regr\")\n",
        "axes[1].set_xlabel(\"Feature 1\")\n",
        "axes[1].set_ylabel(\"Feature 2\")\n",
        "plt.tight_layout()\n",
        "# plt.save"
      ],
      "metadata": {
        "id": "lhKEjgDkf6dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2 Randomly shuffle the datasets (i.e. the order of the rows) and split them each into 70% (for training) and 30% (for testing)."
      ],
      "metadata": {
        "id": "I58Nr1Bt59EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specific Imports\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "JQvnUh_5l3l6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df1.iloc[:, :-1].values, df1.iloc[:, -1].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "yGqt4oee6LrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3"
      ],
      "metadata": {
        "id": "rNm5n4H86VwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specific Imports\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib.colors import ListedColormap"
      ],
      "metadata": {
        "id": "OWEy1JWk59Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a. Build a k-NN classifier with k = 3 for dataset w3classif.csv and find the training and test loss (i.e. misclassification rate)."
      ],
      "metadata": {
        "id": "rO7mn1jb8KXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_predict_knn(X_train, y_train, X_test, y_test, n_neighbors=3):\n",
        "  # Make a KNN Classifier and Train it on Dataset\n",
        "  knn = KNeighborsClassifier(n_neighbors=3)\n",
        "  knn.fit(X_train, y_train)\n",
        "\n",
        "  # Making Predictions\n",
        "  y_train_pred = knn.predict(X_train)\n",
        "  y_test_pred = knn.predict(X_test)\n",
        "  return knn, y_train_pred, y_test_pred\n",
        "\n",
        "def evaluate_knn(y_train, y_train_pred, y_test, y_test_pred):\n",
        "  # Misclassification rate\n",
        "  train_loss = 1 - accuracy_score(y_train, y_train_pred)\n",
        "  test_loss = 1 - accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "  print(f\"\"\"\n",
        "  Misclassification Rate (in %):\n",
        "    Train Loss: {train_loss*100}%\n",
        "    Test Loss: {test_loss*100}%\n",
        "  \"\"\")\n",
        "\n",
        "knn, y_train_pred, y_test_pred = train_and_predict_knn(X_train, y_train, X_test, y_test)\n",
        "evaluate_knn(y_train, y_train_pred, y_test, y_test_pred)"
      ],
      "metadata": {
        "id": "l9TgmlxE6fg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b. Plot the decision regions for your classifier together with the training and/or test data points."
      ],
      "metadata": {
        "id": "sIQk0_XD8MdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_regions(X, y, X_train, y_train, X_test, y_test):\n",
        "  knn, _, _ = train_and_predict_knn(X_train, y_train, X_test, y_test)\n",
        "\n",
        "  # Finding Ranges of Feature1 (f1) and Feature2 (f2), and a feature space\n",
        "  f1_min, f1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "  f2_min, f2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "  f1r, f2r = np.meshgrid(np.arange(f1_min, f1_max, 0.1),\n",
        "                         np.arange(f2_min, f2_max, 0.1))\n",
        "\n",
        "  # Predicting class labels for grid points\n",
        "  Z = knn.predict(np.c_[f1r.ravel(), f2r.ravel()])\n",
        "  Z = Z.reshape(f1r.shape)\n",
        "\n",
        "  # Plotting Decision Boundaries\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  plt.contourf(f1r, f2r, Z, alpha=0.3,\n",
        "               cmap=ListedColormap(['red', 'blue']))\n",
        "  plt.scatter(X_train[:, 0], X_train[:, 1],\n",
        "              c=y_train, cmap=ListedColormap(['darkred', 'darkgreen', 'darkblue']),\n",
        "              edgecolor='k', label='Train')\n",
        "  plt.scatter(X_test[:, 0], X_test[:, 1],\n",
        "              c=y_test, cmap=ListedColormap(['pink', 'lightgreen', 'lightblue']),\n",
        "              edgecolor='k', marker='s', label='Test')\n",
        "  plt.title('k-NN Decision Regions')\n",
        "  plt.xlabel('Feature 1')\n",
        "  plt.ylabel('Feature 2')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "plot_decision_regions(X, y, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "AVKDD4J-8QHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Experiment with different k values and see how it affects the loss values and the decision regions."
      ],
      "metadata": {
        "id": "x15UEqm38QXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1,3):\n",
        "  knn, y_train_pred, y_test_pred = train_and_predict_knn(X_train, y_train, X_test, y_test, n_neighbors=i)\n",
        "  print(f\"{color.YELLOW_BOLD}Evaluation and Decision Regions for {i}NN{color.END}\")\n",
        "  evaluate_knn(y_train, y_train_pred, y_test, y_test_pred)\n",
        "  plot_decision_regions(X, y, X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EdRH6pjL8Wij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **References**:\n",
        "1. “matplotlib.pyplot.subplots — Matplotlib 3.6.0 documentation,” matplotlib.org. https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html\n",
        "2. scikit-learn, “sklearn.neighbors.KNeighborsClassifier — scikit-learn 0.22.1 documentation,” Scikit-learn.org, 2019. https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "3."
      ],
      "metadata": {
        "id": "HniGXXIavxnp"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}