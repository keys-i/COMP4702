{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2703785",
      "metadata": {
        "id": "a2703785"
      },
      "source": [
        "# Week 7: Multilayer Perceptron Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16b039ff",
      "metadata": {
        "id": "16b039ff"
      },
      "source": [
        "### Aims:\n",
        "* To gain some experience in constructing multilayer perceptron networks and solving problems\n",
        "with them.\n",
        "* To produce some assessable work for this subject."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1f6ce1",
      "metadata": {
        "id": "1f1f6ce1"
      },
      "source": [
        "### Procedure:\n",
        "Some of the questions in this Prac use python libraries, in particular the PyTorch library\n",
        "(https://pytorch.org/) developed by Meta AI$^1$. If you are not familiar with python it should still be\n",
        "possible to work through the prac questions using the examples/partial code provided."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$^{\\text{1 Note that this is originally based on Torch, developed in Lua and first released in 2002 (https://en.wikipedia.org/wiki/Torch_(machine_learning)).}}$$\n"
      ],
      "metadata": {
        "id": "Fr02hUNhDlpZ"
      },
      "id": "Fr02hUNhDlpZ"
    },
    {
      "cell_type": "markdown",
      "id": "4b0635b5",
      "metadata": {
        "id": "4b0635b5"
      },
      "source": [
        "### Question 1: The XOR dataset is a classic toy problem in the history of neural networks. Use the code provided to try and train a 2-2-1 MLP to learn XOR. Use the code to verify for yourself the following key points:\n",
        "\n",
        "* The result of the training is sensitive to the initialization of the weights\n",
        "* The result of the training is sensitive to the learning rate.\n",
        "* The result of the training is sensitive to the number of epochs/weight updates.\n",
        "* The result of the training is sensitive to the scale of the (input) data.\n",
        "\n",
        "Hint: think about what variable(s) you need to look at to verify these things. Add some lines to\n",
        "print out and/or plot some of the variables before/during/after training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1b9ae41",
      "metadata": {
        "id": "b1b9ae41"
      },
      "source": [
        "\n",
        "## Introduction to PyTorch\n",
        "### Question 2: On the course website you will find a link to a google colab notebook. Read and step through this notebook to see how to create and train MLPs in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Data\n",
        "### Question 3: Train an MLP on the MNIST data (available on the course website). You will need to make several “design choices” to do this. Make sure you split your dataset into a training, validation and test set. Your answer for this question should describe the design choices you made to create and train your network and a report of your results. You can also include error graphs or any other output you feel is useful.\n",
        "\n",
        "* **Python Help**: some example code is provided using either sklearn or pytorch.\n",
        "* **Matlab Help**: run the Neural Network Pattern Recognition tool from the command line(nprtool). You can import a dataset (e.g. cancer) and step through the creation, training and evaluation of a neural network inside the tool. You can also generate code from this process. You could then modify this code to apply to the MNIST data (or go the opposite way: import the MNIST data into the GUI tool).<br /><br />\n",
        "There are other apps that could be used: Classification Learner or Deep Network Designer. <br/><br />\n",
        "The following code may be used in formatting the data. You will have to change it slightly to use the test and train dataset.\n",
        "```matlab\n",
        "load('mnist_train.mat');\n",
        "labels = [];\n",
        "for i = 1:10\n",
        "labels = [labels, train_labels == i];\n",
        "end\n",
        "```"
      ],
      "metadata": {
        "id": "RoUpgmUE_Z4D"
      },
      "id": "RoUpgmUE_Z4D"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4: The weight matrix and bias vector in first hidden layer apply a linear transformation\n",
        "before the activation function is applied. The ith row of the weight vector contains the weightings\n",
        "of the 728 inputs to the ith neuron. The importance of each input on the ith neuron can then be\n",
        "visualised by drawing the ith row of the weight vector as a greyscale image. Visualise 9 rows of the\n",
        "weight matrix. <br />\n",
        "- **Python help**: see the example code provided.<br />\n",
        "- **Matlab help**:\n",
        "  * One way to get the weight matrix of the trained network is to click “Matlab Matrix Only Function” in the “Deploy Solution” dialogue of nprtool. You will need to copy and paste IW1_1 and x1_step1_keep into a new script.\n",
        "  * Each row will have to be normalised so that every element is in [0,1].\n",
        "  * reshape() and imshow() may be useful."
      ],
      "metadata": {
        "id": "NPGWC-gPCLhx"
      },
      "id": "NPGWC-gPCLhx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extra Topics to Note/Explore:\n",
        "* Numerical methods (like gradient descent) typically require termination conditions. This could be a simple as training for a fixed number of epochs, however other things could also be considered (e.g. stopping if the magnitude of the gradient drops below some small value, or if the loss has not decreased by more than some small value in the last few steps. These choices are effectively hyperparameters of the configuration of any training experiment. Software libraries will typically have some default settings.\n",
        "* You might like to explore using different training algorithms beyond stochastic gradient\n",
        "descent (e.g. Adam)."
      ],
      "metadata": {
        "id": "tOpOTFwLCU3K"
      },
      "id": "tOpOTFwLCU3K"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}